{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47ab45f-bf9f-4020-bc4b-99e8e0d7e048",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Prepare Dataset\n",
    "- Install Packages\n",
    "- Import Packages\n",
    "- Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414aa7b5-fbe5-428f-a3e6-7e252558ed4b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5d2637b-f67f-47da-98c2-bf89a59abd16",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-19T06:29:05.680940500Z",
     "start_time": "2023-12-19T06:29:05.371695400Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install copernicus_marine_client\n",
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43cfc38-1225-4550-9333-5a72e6ff0049",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-19T06:29:05.680940500Z",
     "start_time": "2023-12-19T06:29:05.386590800Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install keras-tuner --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce9bfc-38b6-42a2-b4b1-81015e19c2a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2dacf9d-9826-4e3e-b101-40a766867b79",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-19T06:29:07.038048Z",
     "start_time": "2023-12-19T06:29:05.403674500Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cbook' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\12603\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [3]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mglob\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m glob\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py:109\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    105\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpackaging\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mversion\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m parse \u001B[38;5;28;01mas\u001B[39;00m parse_version\n\u001B[0;32m    107\u001B[0m \u001B[38;5;66;03m# cbook must import matplotlib only within function\u001B[39;00m\n\u001B[0;32m    108\u001B[0m \u001B[38;5;66;03m# definitions, so it is safe to import from it here.\u001B[39;00m\n\u001B[1;32m--> 109\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, _version, cbook, docstring, rcsetup\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MatplotlibDeprecationWarning, sanitize_sequence\n\u001B[0;32m    111\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mplDeprecation  \u001B[38;5;66;03m# deprecated\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\rcsetup.py:25\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m---> 25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _api, cbook\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcbook\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ls_mapper\n\u001B[0;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolors\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Colormap, is_color_like\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'cbook' from partially initialized module 'matplotlib' (most likely due to a circular import) (C:\\Users\\12603\\anaconda3\\lib\\site-packages\\matplotlib\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "import copernicus_marine_client as copernicusmarine\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b025463-ad45-468f-965b-79bfae57ce4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Download Data  \n",
    "- Download data from source website form two dataset_id\n",
    "- Merge two dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42bcb7-4509-4ebc-ba01-e6493a6077dc",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.034312100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download data from source website form two dataset_id\n",
    "copernicusmarine.login(\"xfeng4\",\"Sf1260358662@\") #login, overwrite it ? [y/N]: y\n",
    "\n",
    "#field name in dataset\n",
    "data_variables = [\"sithick\",\"siconc\",\"thetao\",\"bottomT\",\"so\", \"usi\", \"vsi\",\"uo\",\"vo\"]\n",
    "\n",
    "#create folder to store raw data\n",
    "cwd = os.getcwd()\n",
    "folder = \"sea ice thickness dataset\"\n",
    "site = \"greenland\" #create dictionary: {site: lat and long range}\n",
    "folder_path = os.path.join(cwd, folder, site) \n",
    "os.makedirs(folder_path, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb6e527-4534-4d4c-b344-9f1d24dcec0a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.034312100Z"
    }
   },
   "outputs": [],
   "source": [
    "#first dataset\n",
    "for var in data_variables:\n",
    "    output_file = os.path.join(folder_path, f\"{var}_id1.nc\")\n",
    "    if os.path.exists(output_file):\n",
    "        pass\n",
    "    else:\n",
    "        copernicusmarine.subset(\n",
    "          dataset_id=\"cmems_mod_glo_phy_my_0.083deg_P1D-m\",\n",
    "          variables=[var],\n",
    "          minimum_longitude=-58.133,\n",
    "          maximum_longitude=-57.718,\n",
    "          minimum_latitude=82.892,\n",
    "          maximum_latitude=83.307,\n",
    "          start_datetime=\"1993-01-01T00:00:00\",\n",
    "          end_datetime=\"2021-06-30T23:59:59\",\n",
    "          output_filename=output_file)\n",
    "\n",
    "#second dataset ID\n",
    "for var in data_variables:\n",
    "    output_file = os.path.join(folder_path, f\"{var}_id2.nc\")\n",
    "    if os.path.exists(output_file):\n",
    "        pass\n",
    "    else:\n",
    "        copernicusmarine.subset(\n",
    "          dataset_id=\"cmems_mod_glo_phy_myint_0.083deg_P1D-m\",\n",
    "          variables=[var],\n",
    "          minimum_longitude=-58.133,\n",
    "          maximum_longitude=-57.718,\n",
    "          minimum_latitude=82.892,\n",
    "          maximum_latitude=83.307,\n",
    "          start_datetime=\"2021-07-01T00:00:00\",\n",
    "          end_datetime=\"2023-07-31T23:59:59\",\n",
    "          output_filename=os.path.join(folder_path, f\"{var}_id2.nc\"))\n",
    "        \n",
    "print(\"down\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb54ea9-9f23-49df-95b2-2de9939afa31",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.035540700Z"
    }
   },
   "outputs": [],
   "source": [
    "#Merge two dataset\n",
    "os.makedirs(os.path.join(folder_path, \"Merged\"), exist_ok=True) \n",
    "\n",
    "for var in data_variables:\n",
    "    output_file = os.path.join(folder_path, \"Merged\", f\"{var}_merged.nc\")\n",
    "    if os.path.exists(output_file):\n",
    "        pass\n",
    "    else:\n",
    "        data_id1_file = os.path.join(folder_path, f\"{var}_id1.nc\")\n",
    "        data_id2_file = os.path.join(folder_path, f\"{var}_id2.nc\")\n",
    "        data_id1_xr = xr.open_dataset(data_id1_file)\n",
    "        data_id2_xr = xr.open_dataset(data_id2_file)\n",
    "        data_xr = xr.merge([data_id1_xr, data_id2_xr])\n",
    "        data_file = os.path.join(folder_path, \"Merged\", f\"{var}_merged.nc\")\n",
    "        data_xr.to_netcdf(data_file)\n",
    "        \n",
    "print(\"down\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19826406-26dc-4dce-ab40-69bdb3eb2e3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Process Data\n",
    "- read as xarray, drop \"depth\" dimension, and merge xrray\n",
    "- normalization\n",
    "- flat data for NN\n",
    "- time slider for LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9874809b-ea77-4b18-a403-f41bdd93c1c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### Read as xarray, drop \"depth\" dimension, and merge xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b57ecc-19de-4b23-800a-083c4a63c8e4",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.036542900Z"
    }
   },
   "outputs": [],
   "source": [
    "#extract training_data\n",
    "#create variable for time slice\n",
    "training_data = {}\n",
    "for var in data_variables:\n",
    "    data_file = os.path.join(folder_path, \"Merged\", f\"{var}_merged.nc\")\n",
    "    selection_dataset = xr.open_dataset(data_file)\n",
    "    selection_dataset_time = selection_dataset.sel(time=slice('1993-01-01', '2018-12-31'))\n",
    "\n",
    "    if 'depth' in selection_dataset_time[var].dims: #drop \"depth\" if it exists\n",
    "        selection_dataset_time[var] = selection_dataset_time[var].mean(dim='depth')\n",
    "        selection_dataset_time = selection_dataset_time.drop_vars(\"depth\")\n",
    "        check_nans = selection_dataset_time[var].isnull().any()\n",
    "        if check_nans: #complete data via linear interpolate\n",
    "            selection_dataset_time[var] = selection_dataset_time[var].interpolate_na(dim='time')\n",
    "    else:\n",
    "        check_nans = selection_dataset_time[var].isnull().any()\n",
    "        if check_nans:\n",
    "            selection_dataset_time[var] = selection_dataset_time[var].interpolate_na(dim='time')\n",
    "\n",
    "    training_data[var] = selection_dataset_time\n",
    "\n",
    "#check if dimensions same: only keep time, lat and lon\n",
    "dimensions = None\n",
    "for var, dataset in training_data.items():\n",
    "    if dimensions is None:\n",
    "        dimensions = dataset.dims\n",
    "    elif dimensions != dataset.dims:\n",
    "        raise ValueError(f\"Dimension mismatch in variable {var}\")\n",
    "\n",
    "train = xr.merge([training_data[var] for var in data_variables])\n",
    "\n",
    "#get input and output variables \n",
    "x_train_xr = train[[\"siconc\", \"thetao\", \"bottomT\", \"so\", \"usi\", \"vsi\", \"uo\", \"vo\"]]\n",
    "y_train_xr = train[\"sithick\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0466ad8a-0ce7-44b3-85af-4caa40d27125",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.036542900Z"
    }
   },
   "outputs": [],
   "source": [
    "#extract testing_data\n",
    "#write a function for extract and merge\n",
    "testing_data = {}\n",
    "for var in data_variables:\n",
    "    data_file = os.path.join(folder_path, \"Merged\", f\"{var}_merged.nc\")\n",
    "    selection_dataset = xr.open_dataset(data_file)\n",
    "    selection_dataset_time = selection_dataset.sel(time=slice('2019-01-01', '2023-07-31'))\n",
    "\n",
    "    if 'depth' in selection_dataset_time[var].dims:\n",
    "        selection_dataset_time[var] = selection_dataset_time[var].mean(dim='depth')\n",
    "        selection_dataset_time = selection_dataset_time.drop_vars(\"depth\")\n",
    "        check_nans = selection_dataset_time[var].isnull().any()\n",
    "        if check_nans:\n",
    "            selection_dataset_time[var] = selection_dataset_time[var].interpolate_na(dim='time')\n",
    "    else:\n",
    "        check_nans = selection_dataset_time[var].isnull().any()\n",
    "        if check_nans:\n",
    "            selection_dataset_time[var] = selection_dataset_time[var].interpolate_na(dim='time')\n",
    "\n",
    "    testing_data[var] = selection_dataset_time\n",
    "    \n",
    "dimensions = None\n",
    "for var, dataset in testing_data.items():\n",
    "    if dimensions is None:\n",
    "        dimensions = dataset.dims\n",
    "    elif dimensions != dataset.dims:\n",
    "        raise ValueError(f\"Dimension mismatch in variable {var}\")\n",
    "\n",
    "test = xr.merge([testing_data[var] for var in data_variables])\n",
    "\n",
    "x_test_xr = test[[\"siconc\", \"thetao\", \"bottomT\", \"so\", \"usi\", \"vsi\", \"uo\", \"vo\"]]\n",
    "y_test_xr = test[\"sithick\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5639fb-0cee-450b-9e69-ec81421b1c42",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742e8ac-39ed-40a4-ab30-530dc38c0eb3",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.038048Z"
    }
   },
   "outputs": [],
   "source": [
    "meanstd_inputs = {}\n",
    "predictors = [\"siconc\", \"thetao\", \"bottomT\", \"so\", \"usi\", \"vsi\", \"uo\", \"vo\"]\n",
    "\n",
    "def normalize(data, var, meanstd_dict):\n",
    "    mean = meanstd_dict[var][0]\n",
    "    std = meanstd_dict[var][1]\n",
    "    return (data - mean)/std\n",
    "\n",
    "for var in predictors:\n",
    "    meanstd_inputs[var] = (x_train_xr[var].data.mean(), x_train_xr[var].data.std())\n",
    "    \n",
    "# normalize each variables\n",
    "for var in predictors:\n",
    "    # training set\n",
    "    var_dims   = x_train_xr[var].dims\n",
    "    x_train_xr = x_train_xr.assign({var: (var_dims, normalize(x_train_xr[var].data, var, meanstd_inputs))})\n",
    "    \n",
    "    # test set\n",
    "    var_dims  = x_test_xr[var].dims\n",
    "    x_test_xr = x_test_xr.assign({var: (var_dims, normalize(x_test_xr[var].data, var, meanstd_inputs))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db79ddd-c1c7-4021-b943-ef144128d77c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.038048Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_np  = x_train_xr.to_array().transpose('time', 'latitude', 'longitude', 'variable').data\n",
    "x_test_np  = x_test_xr.to_array().transpose('time', 'latitude', 'longitude', 'variable').data\n",
    "\n",
    "y_train_np = y_train_xr.data\n",
    "y_test_np = y_test_xr.data\n",
    "\n",
    "print(x_train_np.shape, x_test_np.shape, y_train_np.shape, y_test_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae2221-a9de-4f50-bbd9-941ccf350659",
   "metadata": {},
   "source": [
    "##### Flat data for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165f57f-aa46-46cc-afbc-ca58c6fcde19",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.039558100Z"
    }
   },
   "outputs": [],
   "source": [
    "#convert model to 1D for NN\n",
    "x_train_flattened = x_train_np.reshape(x_train_np.shape[0], -1)  # x_train.shape[0] is the number of samples\n",
    "y_train_flattened = y_train_np.reshape(y_train_np.shape[0], -1)\n",
    "x_test_flattened = x_test_np.reshape(x_test_np.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae3935f-eb69-4cfb-a3f0-69a3e8d25168",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Add time slider for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2d52ab-bd76-437c-8cbb-7ac206a1d01f",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.040564600Z"
    }
   },
   "outputs": [],
   "source": [
    "#adjust dataset to feed LSTM\n",
    "slider = 5\n",
    "x_train_slider = np.array([x_train_np[i:i+slider] for i in range(0, x_train_np.shape[0]-slider+1)])\n",
    "y_train_slider = np.array([y_train_np[i+slider-1] for i in range(0, y_train_np.shape[0]-slider+1)])\n",
    "\n",
    "x_test_slider = np.array([x_test_np[i:i+slider] for i in range(0, x_test_np.shape[0]-slider+1)])\n",
    "\n",
    "print(x_train_np.shape, x_train_slider.shape)\n",
    "print(y_train_np.shape, y_train_slider.shape)\n",
    "print(x_test_np.shape, x_test_slider.shape)\n",
    "\n",
    "#stack them to get shape (n_timepoints, n_slider, features)\n",
    "input_shape = (5, 5, 5, 8)  # time slider, lat, long, features\n",
    "output_shape = (5, 5)    # lat, long\n",
    "\n",
    "# Flatten the spatial dimensions for the LSTM input\n",
    "flattened_spatial_dim = input_shape[1] * input_shape[2] * input_shape[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa6ea1a-1367-49c3-8db5-1f2a1a3c0f3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train the model\n",
    "- NN\n",
    "- NN + keras_tuner, always excess the limit\n",
    "- CNN + keras_tuner\n",
    "- LSTM + keras_tuner\n",
    "- CNN LSTM + keras_tuner  \n",
    "*how to do feature selection?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945ad29-11b1-4b44-9b4f-10877eb75985",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322d956e-2064-41d7-af42-09b915266650",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-19T06:29:07.057663700Z",
     "start_time": "2023-12-19T06:29:07.042070400Z"
    }
   },
   "outputs": [],
   "source": [
    "n_neuron       = 64 #the number of n_neuron\n",
    "activation     = 'relu' #activation methods\n",
    "num_epochs     = 50 #go through how many times weight adjusted\n",
    "learning_rate  = 0.001 #related to optimize\n",
    "minibatch_size = 64 #the way to optimize\n",
    "model_num      = 1\n",
    "N_layers       = 2 # number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23bc1e2-6ec5-45c6-89b3-16c155fc4da1",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.043072900Z"
    }
   },
   "outputs": [],
   "source": [
    "# mse\n",
    "model_mse = Sequential()\n",
    "model_mse.add(Dense(n_neuron,  activation=activation, name=\"hidden_layer_1\" ,input_shape=(x_train_flattened.shape[1],)))\n",
    "for n in range(N_layers-1):\n",
    "    model_mse.add(Dense(n_neuron,  activation=activation, name=\"hidden_layer_\"+str(n+2))) \n",
    "model_mse.add(Dense(y_train_flattened.shape[1],  activation='linear', name=\"output_layer\")) # the output layer\n",
    "model_mse.compile(loss='mse',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "model_mse.summary()\n",
    "\n",
    "#mae\n",
    "model_mae = Sequential()\n",
    "model_mae.add(Dense(n_neuron,  activation=activation, name=\"hidden_layer_1\" ,input_shape=(x_train_flattened.shape[1],)))\n",
    "for n in range(N_layers-1):\n",
    "    model_mae.add(Dense(n_neuron,  activation=activation, name=\"hidden_layer_\"+str(n+2))) \n",
    "model_mae.add(Dense(y_train_flattened.shape[1],  activation='linear', name=\"output_layer\")) # the output layer\n",
    "model_mae.compile(loss='mae',optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "model_mae.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abbdf11-5e7e-4f13-910a-695c5cc2aa1d",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.044073500Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "#mse\n",
    "history_mse = model_mse.fit(x_train_flattened, y_train_flattened, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])\n",
    "#mae\n",
    "history_mae = model_mae.fit(x_train_flattened, y_train_flattened, \n",
    "                            batch_size      = minibatch_size,\n",
    "                            epochs          = num_epochs,\n",
    "                            validation_split= 0.2, \n",
    "                            verbose         = 1,\n",
    "                            callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a2506-3a89-4ba7-86aa-abbcb63aa9f7",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.045072900Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history_mse.history['loss'])\n",
    "plt.plot(history_mse.history['val_loss'])\n",
    "plt.title('loss: Mean Squared Error', fontsize=12)\n",
    "plt.ylabel('mse')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], fontsize=10, loc='upper right')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history_mae.history['loss'])\n",
    "plt.plot(history_mae.history['val_loss'])\n",
    "plt.title('loss: Mean Absolute Error', fontsize=12)\n",
    "plt.ylabel('mae')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], fontsize=10, loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fb1fdc-9c80-4ec7-a08b-47b9c172d8bb",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.046072800Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(cwd,'saved_model')\n",
    "os.makedirs(model_path, exist_ok=True)\n",
    "model_mse.save(os.path.join(model_path,'NN_mse_model.h5'))\n",
    "model_mae.save(os.path.join(model_path, 'NN_mae_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186001d7-ca83-4f1c-b27e-69b460591009",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### NN + keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec104e44-57fa-4258-be6b-c53e58318305",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.047072800Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train_flattened = x_train_np.reshape(x_train.shape[0], -1)  # x_train.shape[0] is the number of samples\n",
    "y_train_flattened = y_train_np.reshape(y_train.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de753466-42b0-4073-950f-cd2140c52af3",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.048073200Z"
    }
   },
   "outputs": [],
   "source": [
    "N_layers = 2\n",
    "minibatch_size = 64\n",
    "num_epochs = 80\n",
    "\n",
    "def NN_model(hp):\n",
    "    # options for hyperparameters\n",
    "    hp_neuron = hp.Choice('neurons', values=[16, 32])\n",
    "    hp_kernel_size = hp.Choice('kernel_size', values=[2, 3, 4, 5])\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    \n",
    "    model_mse = Sequential()\n",
    "    model_mse.add(Dense(hp_neuron*x_train_flattened.shape[1],  activation=hp_activation, name=\"hidden_layer_1\" ,input_shape=(x_train_flattened.shape[1],)))\n",
    "    for n in range(N_layers-1):\n",
    "        model_mse.add(Dense(hp_neuron*x_train_flattened.shape[1],  activation=activation, name=\"hidden_layer_\"+str(n+2))) \n",
    "    model_mse.add(Dense(y_train_flattened.shape[1],  activation='linear', name=\"output_layer\")) # the output layer\n",
    "    model_mse.compile(loss=\"mse\",optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    model_mse.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eff8e8-b188-4da1-a0e3-8aecb0abf97e",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.049579800Z"
    }
   },
   "outputs": [],
   "source": [
    "random_tuner = keras_tuner.RandomSearch(NN_model, \n",
    "                                        max_trials=n_trials,\n",
    "                                        seed=5,\n",
    "                                        objective='val_loss', \n",
    "                                        max_retries_per_trial=0,\n",
    "                                        directory='random_search_NN', \n",
    "                                        project_name='random_search_NN_10trials')\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "random_tuner.search(x_train_flattened, y_train_flattened,\n",
    "                    batch_size = minibatch_size,\n",
    "                    epochs = num_epochs, \n",
    "                    validation_split = 0.2, \n",
    "                    callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130c776-1cef-40fd-8819-a1df83a60295",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### CNN + Keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6843ab9-ba25-4e07-949f-b8c3a494004d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.050589500Z"
    }
   },
   "outputs": [],
   "source": [
    "minibatch_size = 64\n",
    "num_epochs     = 80\n",
    "n_trials       = 10   # total number of trials in hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8777505-b248-4456-a841-6b0eb1774f34",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.051589300Z"
    }
   },
   "outputs": [],
   "source": [
    "def CNN_model(hp):\n",
    "    \n",
    "    # options for hyperparameters\n",
    "    hp_filters = hp.Choice('filters', values=[16,32,64])\n",
    "    hp_kernel_size = hp.Choice('kernel_size', values=[2,3,4,5])\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    hp_loss = hp.Choice('loss', values=['mse','mae'])\n",
    "    \n",
    "    # build CNN models according to the hyperparameters\n",
    "    model = keras.models.Sequential()\n",
    "\n",
    "    model.add(Dense(hp_filters*5*5, input_shape=x_train.shape[1:], activation=hp_activation)) \n",
    "    \n",
    "    model.add(Conv2DTranspose(filters=hp_filters, kernel_size=hp_kernel_size, \n",
    "                              activation=hp_activation, strides=1, padding='same')) # shape: (5,5,filters)\n",
    "    \n",
    "    model.add(Conv2DTranspose(filters=hp_filters, kernel_size=hp_kernel_size, \n",
    "                              activation=hp_activation, strides=1, padding='same')) # shape: (48,72,filters)\n",
    "    \n",
    "    model.add(Conv2DTranspose(filters=hp_filters, kernel_size=hp_kernel_size, \n",
    "                              activation=hp_activation, strides=1, padding='same')) # shape: (96,144,filters)\n",
    "    \n",
    "    model.add(Conv2DTranspose(filters=1, kernel_size=hp_kernel_size, \n",
    "                              activation=\"linear\", padding=\"same\")) # shape: (96,144,1)\n",
    "    \n",
    "    model.compile(loss=hp_loss, optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1561e4-767a-44d6-96f1-d8c2541a3b31",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.051589300Z"
    }
   },
   "outputs": [],
   "source": [
    "random_tuner = keras_tuner.RandomSearch(CNN_model, \n",
    "                                        max_trials=n_trials,\n",
    "                                        seed=5,\n",
    "                                        objective='val_loss', \n",
    "                                        max_retries_per_trial=0,\n",
    "                                        max_consecutive_failed_trials=3,\n",
    "                                        directory='random_search', \n",
    "                                        project_name='random_search_CNN_10trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a39edb-504d-4b30-bb4d-a270f3f62814",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.052663600Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69854b2-8a5a-4c42-8158-f45e7bf803a4",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.053663800Z"
    }
   },
   "outputs": [],
   "source": [
    "random_tuner.search(x_train_np, y_train_np,\n",
    "                    batch_size = minibatch_size,\n",
    "                    epochs = num_epochs, \n",
    "                    validation_split = 0.2, \n",
    "                    callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15119b1-e2b9-40ac-80ae-97b8786aff99",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.054663200Z"
    }
   },
   "outputs": [],
   "source": [
    "best_model = random_tuner.get_best_models()[0]\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f640435-7259-4127-975a-b3d64171ea0a",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.055663700Z"
    }
   },
   "outputs": [],
   "source": [
    "all_hps = random_tuner.get_best_hyperparameters(num_trials=n_trials)\n",
    "\n",
    "# print the hyperparameters of the top 3 tuning trials\n",
    "for it in range(3):\n",
    "    print(\"Ranking #\", str(it+1).zfill(1), \"of best tuning, total trials = \", str(n_trials))\n",
    "    print(all_hps[it].values)\n",
    "    print(\"=============================================================================================\")\n",
    "\n",
    "\n",
    "# get the hyperparameters of the best tuning trial\n",
    "best_hps = random_tuner.get_best_hyperparameters(num_trials=n_trials)[0]\n",
    "\n",
    "# get the hyperparameters of the second ranking tuning trial\n",
    "top2_hps = random_tuner.get_best_hyperparameters(num_trials=n_trials)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513e3be4-9906-48e2-a368-4e16b12908cd",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.056663600Z"
    }
   },
   "outputs": [],
   "source": [
    "## Build the model with the optimal hyperparameters\n",
    "# ---------- best tuning ----------\n",
    "model_best = random_tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# ---------- ranking #2 tuning ----------\n",
    "model_top2 = random_tuner.hypermodel.build(top2_hps)\n",
    "\n",
    "\n",
    "\n",
    "## re-train the model\n",
    "# ---------- best tuning ----------\n",
    "history_best = model_best.fit(x_train, y_train,\n",
    "                              batch_size      = minibatch_size,\n",
    "                              epochs          = num_epochs,\n",
    "                              validation_split= 0.2, \n",
    "                              verbose         = 1,\n",
    "                              callbacks       = [early_stop])\n",
    "# ---------- ranking #2 tuning ----------\n",
    "history_top2 = model_top2.fit(x_train, y_train,\n",
    "                              batch_size      = minibatch_size,\n",
    "                              epochs          = num_epochs,\n",
    "                              validation_split= 0.2, \n",
    "                              verbose         = 1,\n",
    "                              callbacks       = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64dbe51-0290-4479-b065-6aac55e18282",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-12-19T06:29:07.124861800Z",
     "start_time": "2023-12-19T06:29:07.057663700Z"
    }
   },
   "outputs": [],
   "source": [
    "## set up title and ylabel strings\n",
    "# ---------- best tuning ----------\n",
    "title_best = 'loss: '+str(best_hps['loss'])\n",
    "ylabel_best = best_hps['loss']\n",
    "# ---------- ranking #2 tuning ----------\n",
    "title_top2 = 'loss: '+str(top2_hps['loss'])\n",
    "ylabel_top2 = top2_hps['loss']\n",
    "\n",
    "\n",
    "## plot history\n",
    "# ---------- best tuning ----------\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_best.history['loss'])\n",
    "plt.plot(history_best.history['val_loss'])\n",
    "plt.title(title_best, fontsize=12)\n",
    "plt.ylabel(ylabel_best)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], fontsize=10, loc='upper right')\n",
    "\n",
    "# ---------- ranking #2 tuning ----------\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_top2.history['loss'])\n",
    "plt.plot(history_top2.history['val_loss'])\n",
    "plt.title(title_top2, fontsize=12)\n",
    "plt.ylabel(ylabel_top2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], fontsize=10, loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb9061-ed6b-430b-987d-8f0b510d006c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.058663700Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(cwd,'saved_model')\n",
    "os.makedirs(model_path, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d8d3e-6cf9-4392-9384-6fdde19bd32a",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.058663700Z"
    }
   },
   "outputs": [],
   "source": [
    "model_best.save(os.path.join(model_path,'CNN_model_randomsearch_10trials_rank1.h5'))\n",
    "model_top2.save(os.path.join(model_path,'CNN_model_randomsearch_10trials_rank2.h5'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5e31a0-42d1-4882-8035-66fb253ebc65",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LSTM + keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf0395-7f43-406a-a6eb-607793b31995",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.059663900Z"
    }
   },
   "outputs": [],
   "source": [
    "# set hyperparameters\n",
    "num_epochs     = 50\n",
    "minibatch_size = 64\n",
    "n_trials = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efb3f0c-66cf-444f-a1e4-066ef2288f97",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.060769800Z"
    }
   },
   "outputs": [],
   "source": [
    "def LSTM_model(hp):\n",
    "\n",
    "    hp_neuron = hp.Choice('filters', values=[32, 64, 128])\n",
    "    hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid'])\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    hp_loss = hp.Choice('loss', values=['mse','mae'])\n",
    "    LSTM_model = Sequential()\n",
    "    LSTM_model.add(Reshape((input_shape[0], flattened_spatial_dim), input_shape=input_shape))\n",
    "    LSTM_model.add(LSTM(hp_neuron, return_sequences=True, activation=hp_activation))\n",
    "    LSTM_model.add(LSTM(hp_neuron, return_sequences=False, activation=hp_activation))\n",
    "    LSTM_model.add(Dense(output_shape[0] * output_shape[1], activation=\"linear\"))\n",
    "    LSTM_model.add(Reshape(output_shape))\n",
    "\n",
    "    LSTM_model.compile(loss=hp_loss, optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate))\n",
    "    return LSTM_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83769b9-127b-4021-8af7-7c73edf27736",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.061769300Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "random_tuner = keras_tuner.RandomSearch(LSTM_model, \n",
    "                                        max_trials=n_trials,\n",
    "                                        seed=5,\n",
    "                                        objective='val_loss', \n",
    "                                        max_retries_per_trial=0,\n",
    "                                        max_consecutive_failed_trials=3,\n",
    "                                        directory='random_search', \n",
    "                                        project_name='random_search_LSTM_12trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1741aae1-fb72-4b82-b015-2a3c5984fb17",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.061769300Z"
    }
   },
   "outputs": [],
   "source": [
    "random_tuner.search(x_train_slider, y_train_slider,\n",
    "                    batch_size = minibatch_size,\n",
    "                    epochs = num_epochs, \n",
    "                    validation_split = 0.2, \n",
    "                    callbacks = [early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43ee9c-3505-42f4-92bf-a5390a0d3612",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.061769300Z"
    }
   },
   "outputs": [],
   "source": [
    "# get the hyperparameters of the best tuning trial\n",
    "best_hps = random_tuner.get_best_hyperparameters(num_trials=n_trials)[0]\n",
    "\n",
    "# get the hyperparameters of the second ranking tuning trial\n",
    "top2_hps = random_tuner.get_best_hyperparameters(num_trials=n_trials)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dcfda8-fe53-4919-837b-06ab80124770",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.063055300Z"
    }
   },
   "outputs": [],
   "source": [
    "## Build the model with the optimal hyperparameters\n",
    "# ---------- best tuning ----------\n",
    "model_best = random_tuner.hypermodel.build(best_hps)\n",
    "\n",
    "# ---------- ranking #2 tuning ----------\n",
    "model_top2 = random_tuner.hypermodel.build(top2_hps)\n",
    "\n",
    "\n",
    "\n",
    "## re-train the model\n",
    "# ---------- best tuning ----------\n",
    "history_best = model_best.fit(x_train_slider, y_train_slider,\n",
    "                              batch_size      = minibatch_size,\n",
    "                              epochs          = num_epochs,\n",
    "                              validation_split= 0.2, \n",
    "                              verbose         = 1,\n",
    "                              callbacks       = [early_stop])\n",
    "# ---------- ranking #2 tuning ----------\n",
    "history_top2 = model_top2.fit(x_train_slider, y_train_slider,\n",
    "                              batch_size      = minibatch_size,\n",
    "                              epochs          = num_epochs,\n",
    "                              validation_split= 0.2, \n",
    "                              verbose         = 1,\n",
    "                              callbacks       = [early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473c9d35-6196-44c4-9379-1ba332814dab",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.064045500Z"
    }
   },
   "outputs": [],
   "source": [
    "## set up title and ylabel strings\n",
    "# ---------- best tuning ----------\n",
    "title_best = 'loss: '+str(best_hps['loss'])\n",
    "ylabel_best = best_hps['loss']\n",
    "# ---------- ranking #2 tuning ----------\n",
    "title_top2 = 'loss: '+str(top2_hps['loss'])\n",
    "ylabel_top2 = top2_hps['loss']\n",
    "\n",
    "\n",
    "## plot history\n",
    "# ---------- best tuning ----------\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_best.history['loss'])\n",
    "plt.plot(history_best.history['val_loss'])\n",
    "plt.title(title_best, fontsize=12)\n",
    "plt.ylabel(ylabel_best)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], fontsize=10, loc='upper right')\n",
    "\n",
    "# ---------- ranking #2 tuning ----------\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_top2.history['loss'])\n",
    "plt.plot(history_top2.history['val_loss'])\n",
    "plt.title(title_top2, fontsize=12)\n",
    "plt.ylabel(ylabel_top2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train loss', 'Val loss'], fontsize=10, loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd304916-451f-4b1e-afd2-4ec7ce128a9d",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.064045500Z"
    }
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(cwd,'saved_model')\n",
    "model_best.save(os.path.join(model_path,'LSTM_model_randomsearch_12trials_rank1.h5'))\n",
    "model_top2.save(os.path.join(model_path,'LSTM_model_randomsearch_12trials_rank2.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648806b-4b01-449c-86b4-4faabe79379a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Test Model\n",
    "- NN\n",
    "- NN + keras tuner\n",
    "- CNN + keras tuner\n",
    "- LSTM + keras tuner\n",
    "- CNN_LSTM + keras tuner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc220af-1834-408d-91b7-42a85d2ea8db",
   "metadata": {},
   "source": [
    "##### NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc2244-8804-48a1-973a-597728632046",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.065045400Z"
    }
   },
   "outputs": [],
   "source": [
    "model_mse = load_model(os.path.join(model_path,'NN_mse_model.h5'))\n",
    "model_mae = load_model(os.path.join(model_path,'NN_mae_model.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca4105-05aa-42d2-98c6-9bb5a0a96e6c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.065045400Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_mse_pre = model_mse.predict(x_test_flattened)\n",
    "y_test_mae_pre = model_mae.predict(x_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfb4cda-b16a-43b6-957f-a983ffa5a672",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.066045200Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_mse_pre = y_test_mse_pre.reshape(y_test_mse_pre.shape[0], 5, 5)\n",
    "y_test_mse_pre = xr.Dataset(coords={'time': x_test_xr.time.values, \n",
    "                                     'latitude': x_test_xr.latitude.values, \n",
    "                                     'longitude': x_test_xr.longitude.values},\n",
    "                             data_vars=dict(sithick=(['time', 'latitude', 'longitude'], y_test_mse_pre)))\n",
    "\n",
    "y_test_mae_pre = y_test_mae_pre.reshape(y_test_mae_pre.shape[0], 5, 5)\n",
    "y_test_mae_pre = xr.Dataset(coords={'time': x_test_xr.time.values, \n",
    "                                     'latitude': x_test_xr.latitude.values, \n",
    "                                     'longitude': x_test_xr.longitude.values},\n",
    "                             data_vars=dict(sithick=(['time', 'latitude', 'longitude'], y_test_mae_pre)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a91f6a-1ac7-42c5-a789-493410e8beea",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.067045400Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_mse_pre"
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "graph"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "850df08f37a7d563"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(y_test_np.shape)\n",
    "y_test_np = xr.Dataset(coords={'time': x_test_xr.time.values, \n",
    "                                     'latitude': x_test_xr.latitude.values, \n",
    "                                     'longitude': x_test_xr.longitude.values},\n",
    "                             data_vars=dict(sithick=(['time', 'latitude', 'longitude'], y_test_np)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.067045400Z"
    }
   },
   "id": "dac0533887c9b4cc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lat = 83.25\n",
    "lon = -57.75\n",
    "#mse\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "y_test_np.sel(latitude=lat, longitude=lon, method='nearest')[\"sithick\"].plot(ax=ax, label='Truth')\n",
    "y_test_mse_pre.sel(latitude=lat, longitude=lon, method='nearest')[\"sithick\"].plot(ax=ax, label='Prediction-mse')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('Sea ice thickness-mse (m)') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "#mae\n",
    "fig, ax = plt.subplots(figsize=(9, 4))\n",
    "y_test_np.sel(latitude=lat, longitude=lon, method='nearest')[\"sithick\"].plot(ax=ax, label='Truth')\n",
    "y_test_mae_pre.sel(latitude=lat, longitude=lon, method='nearest')[\"sithick\"].plot(ax=ax, label='Prediction-mae')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('Sea ice thickness (m)') \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.068045500Z"
    }
   },
   "id": "acdd680c375934c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def global_mean_std_plot(X, label, color, ax, var='sithick'):\n",
    "    weights = np.cos(np.deg2rad(X.latitude))\n",
    "    sithick_mean = X[var].weighted(weights).mean(['latitude', 'longitude']).data\n",
    "    sithick_std = X[var].weighted(weights).std(['latitude', 'longitude']).data\n",
    "    \n",
    "    x = X.time.data\n",
    "\n",
    "    ax.plot(x, sithick_mean, label=label, color=color, linewidth=2)\n",
    "    ax.fill_between(x, sithick_mean+sithick_std, sithick_mean-sithick_std, facecolor=color, alpha=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.068045500Z"
    }
   },
   "id": "e1192208ed4687a7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(9,4))\n",
    "#mse\n",
    "global_mean_std_plot(y_test_np,label='Truth',ax=ax,color='tab:blue')\n",
    "global_mean_std_plot(y_test_mse_pre,label='Prediction-mse',ax=ax,color='tab:orange')\n",
    "\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('Sea Ice Thickness (m)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(9,4))\n",
    "#mae\n",
    "global_mean_std_plot(y_test_np,label='Truth',ax=ax,color='tab:blue')\n",
    "global_mean_std_plot(y_test_mae_pre,label='Prediction-mae',ax=ax,color='tab:orange')\n",
    "\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('Sea Ice Thickness (m)')\n",
    "ax.legend()\n",
    "plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.068045500Z"
    }
   },
   "id": "91b478e2f2d71926"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap as color\n",
    "#mse\n",
    "fig, axes = plt.subplots(figsize=(15, 15), ncols=2, nrows=4)  \n",
    "season = [\"2020-3-20\", \"2020-6-21\", \"2020-9-22\", \"2020-12-21\"]\n",
    "vmin, vmax = -2, 4\n",
    "colors = [\"navy\", \"dodgerblue\",\"white\"]\n",
    "cmap = color.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "for i, date in enumerate(season):\n",
    "    y_test_mse_pre.sithick.sel(time=date).plot(ax=axes[i, 0], vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    y_test_np.sithick.sel(time=date).plot(ax=axes[i, 1], vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    axes[i, 0].set_title(f'Sea Ice Thickness Model Prediction ({date})-mse', fontweight='bold')\n",
    "    axes[i, 1].set_title(f'Sea Ice Thickness Truth ({date})-mse', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#mae\n",
    "fig, axes = plt.subplots(figsize=(8, 8), ncols=2, nrows=4)\n",
    "yrs = [\"2020-3-20\", \"2020-6-21\", \"2020-9-22\", \"2020-12-21\"]\n",
    "vmin, vmax = -2, 4\n",
    "colors = [\"navy\", \"dodgerblue\",\"white\"]\n",
    "cmap = color.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "for i, date in enumerate(yrs):\n",
    "    y_test_mae_pre.sithick.sel(time=date).plot(ax=axes[i, 0], vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    y_test_np.sithick.sel(time=date).plot(ax=axes[i, 1], vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    axes[i, 0].set_title(f'Sea Ice Thickness Model Prediction ({date})-mae', fontweight='bold')\n",
    "    axes[i, 1].set_title(f'Sea Ice Thickness Truth ({date})-mae', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.069550900Z"
    }
   },
   "id": "3f2104ba3f740c5e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### fail to make \n",
    "import matplotlib.animation as animation\n",
    "\n",
    "colors = [\"navy\", \"dodgerblue\", \"white\"]\n",
    "cmap = LinearSegmentedColormap.from_list(\"custom_cmap\", colors)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15), ncols=2, nrows=1)\n",
    "vmin, vmax = -2, 4\n",
    "\n",
    "def init():\n",
    "    ax1.clear()\n",
    "    ax1.set_title('Initializing...')\n",
    "    ax1.set_xlabel('Longitude')\n",
    "    ax1.set_ylabel('Latitude')\n",
    "    \n",
    "    ax2.clear()\n",
    "    ax2.set_title('Initializing...')\n",
    "    ax2.set_xlabel('Longitude')\n",
    "    ax2.set_ylabel('Latitude')\n",
    "    \n",
    "    return ax1, ax2\n",
    "\n",
    "def update(frame):\n",
    "    date = frame\n",
    "    for i in range(1):\n",
    "        axes[i, 0].clear()\n",
    "        axes[i, 1].clear()\n",
    "\n",
    "    y_test_mse_pre.sithick.sel(time=date).plot(ax=ax1, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    ax1.set_title(f'Sea Ice Thickness Model Prediction ({date.strftime(\"%Y-%m-%d\")})-mse', fontweight='bold')\n",
    "\n",
    "    y_test_np.sithick.sel(time=date).plot(ax=ax2, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    ax2.set_title(f'Sea Ice Thickness Truth ({date.strftime(\"%Y-%m-%d\")})-mse', fontweight='bold')\n",
    "\n",
    "    return ax1, ax2\n",
    "\n",
    "dates = pd.date_range('2020-01-01', '2020-12-31', freq='MS')\n",
    "ani = animation.FuncAnimation(fig, update, frames=dates, init_func=init, blit=False)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.070558100Z"
    }
   },
   "id": "17a1bf619132bcc7"
  },
  {
   "cell_type": "markdown",
   "id": "649252cc-345d-4966-8c09-ff22f750a7b5",
   "metadata": {},
   "source": [
    "##### CNN + keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6f03e-0868-41eb-8fd6-ab80ad2e5e6c",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.070558100Z"
    }
   },
   "outputs": [],
   "source": [
    "## reload the saved model\n",
    "model_best = load_model(os.path.join(model_path,'CNN_model_randomsearch_10trials_rank1.h5'))\n",
    "model_top2 = load_model(os.path.join(model_path,'CNN_model_randomsearch_10trials_rank2.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de00ecab-12d7-47ea-8f43-d2db603ac67b",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.072073700Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_best_pre = model_best.predict(x_test_np)\n",
    "y_test_best_pre = y_test_best_pre.reshape(y_test_best_pre.shape[0], 5, 5)\n",
    "y_test_best_pre = xr.Dataset(coords={'time': x_test_xr.time.values, \n",
    "                                     'latitude': x_test_xr.latitude.values, \n",
    "                                     'longitude': x_test_xr.longitude.values},\n",
    "                             data_vars=dict(sithick=(['time', 'latitude', 'longitude'], y_test_best_pre)))\n",
    "# ---------- rank #2 tuning ----------\n",
    "y_test_top2_pre = model_top2.predict(x_test_np)\n",
    "y_test_top2_pre = y_test_top2_pre.reshape(y_test_top2_pre.shape[0], 5, 5)\n",
    "y_test_top2_pre = xr.Dataset(coords={'time': x_test_xr.time.values, \n",
    "                                     'latitude': x_test_xr.latitude.values, \n",
    "                                     'longitude': x_test_xr.longitude.values},\n",
    "                             data_vars=dict(sithick=(['time', 'latitude', 'longitude'], y_test_top2_pre)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f013c-4665-4bcd-b0e6-c1588c4f3e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### LSTM + keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364bf472-9e74-45e1-9a3a-464d6f9c50c2",
   "metadata": {
    "scrolled": true,
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.072073700Z"
    }
   },
   "outputs": [],
   "source": [
    "## reload the saved model\n",
    "model_best = load_model(os.path.join(model_path,'LSTM_model_randomsearch_12trials_rank1.h5'))\n",
    "model_top2 = load_model(os.path.join(model_path,'LSTM_model_randomsearch_12trials_rank2.h5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f299fdc-4b34-4bd8-bde2-1b236db441b0",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.073092700Z"
    }
   },
   "outputs": [],
   "source": [
    "y_test_best_pre = model_best.predict(x_test_slider)\n",
    "y_test_best_pre = y_test_best_pre.reshape(y_test_best_pre.shape[0], 5, 5)\n",
    "y_test_best_pre = xr.Dataset(coords={'time': x_test_xr.time.values[slider-1:], \n",
    "                                     'latitude': x_test_xr.latitude.values, \n",
    "                                     'longitude': x_test_xr.longitude.values},\n",
    "                             data_vars=dict(sithick=(['time', 'latitude', 'longitude'], y_test_best_pre)))\n",
    "# ---------- rank #2 tuning ----------\n",
    "y_test_top2_pre = model_top2.predict(x_test_slider)\n",
    "y_test_top2_pre = y_test_top2_pre.reshape(y_test_top2_pre.shape[0], 5, 5)\n",
    "y_test_top2_pre = xr.Dataset(coords={'time': x_test_xr.time.values[slider-1:], \n",
    "                                     'latitude': x_test_xr.latitude.values, \n",
    "                                     'longitude': x_test_xr.longitude.values},\n",
    "                             data_vars=dict(sithick=(['time', 'latitude', 'longitude'], y_test_top2_pre)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68ea404-ed99-4597-815e-908cc992494a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-19T06:29:07.073616500Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
